# create anaconda virtual machine (https://repo.continuum.io/archive/)
sh /home/hsiehpinghan/Downloads/Anaconda2-5.1.0-Linux-x86_64.sh -b
/home/hsiehpinghan/anaconda2/bin/conda create --name spark-python-2 python=2.7.5 ipykernel

## start jupyter notebook
export ANACONDA_HOME=/home/hsiehpinghan/anaconda2
export PATH=$ANACONDA_HOME/bin:$PATH
export PYSPARK_DRIVER_PYTHON=$ANACONDA_HOME/bin/ipython
export PYSPARK_PYTHON=$ANACONDA_HOME/envs/spark-python-2/bin/python
cd /home/hsiehpinghan/git/python/spark-python-2/notebook
source activate spark-python-2
# local mode
PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS="notebook" MASTER=local[*] pyspark
# yarn mode
PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS="notebook" HADOOP_CONF_DIR=/opt/hadoop-2.7.6/etc/hadoop MASTER=yarn pyspark

# set login without password
https://github.com/hsiehpinghan/example/blob/master/bash-shell-example/example/ssh

# download hadoop
wget http://apache.stu.edu.tw/hadoop/common/hadoop-2.7.6/hadoop-2.7.6.tar.gz -P /tmp/
tar xvfz /tmp/hadoop-2.7.6.tar.gz -C /opt/
vi /etc/profile
	export JAVA_HOME=/opt/jdk1.8.0_151
	export PATH=$JAVA_HOME/bin:$PATH
	export HADOOP_HOME=/opt/hadoop-2.7.6
	export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
vi /opt/hadoop-2.7.6/etc/hadoop/hadoop-env.sh
	export JAVA_HOME=/opt/jdk1.8.0_151
vi /opt/hadoop-2.7.6/etc/hadoop/core-site.xml 
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://localhost:9000</value>
        </property>
    </configuration>
vi /opt/hadoop-2.7.6/etc/hadoop/yarn-site.xml
    <configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.nodemanager.vmem-pmem-ratio</name>
            <value>4</value>
        </property>
    </configuration>
cp /opt/hadoop-2.7.6/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.7.6/etc/hadoop/mapred-site.xml
vi /opt/hadoop-2.7.6/etc/hadoop/mapred-site.xml
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>
vi /opt/hadoop-2.7.6/etc/hadoop/hdfs-site.xml
	<configuration>
	    <property>
	        <name>dfs.replication</name>
	        <value>1</value>
	    </property>
	    <property>
	        <name>dfs.namenode.name.dir</name>
	        <value>file:///home/hsiehpinghan/Desktop/hdfs/namenode</value>
	    </property>
	    <property>
	        <name>dfs.datanode.data.dir</name>
	        <value>file:///home/hsiehpinghan/Desktop/hdfs/datanode</value>
	    </property>
	</configuration>
mkdir -p /home/hsiehpinghan/Desktop/hdfs/namenode
mkdir -p /home/hsiehpinghan/Desktop/hdfs/datanode
hdfs namenode -format

# download scala
wget https://www.scala-lang.org/files/archive/scala-2.11.12.tgz -P /tmp
tar xvfz /tmp/scala-2.11.12.tgz -C /opt/
vi /etc/profile
	export SCALA_HOME=/opt/scala-2.11.12
	export PATH=$SCALA_HOME/bin:$PATH

# download spark
wget http://apache.stu.edu.tw/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz -P /tmp
tar xvfz /tmp/spark-2.3.0-bin-hadoop2.7.tgz -C /opt/
cp /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties.template /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties
vi /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties
	log4j.rootCategory=WARN, console
vi /etc/profile
	export SPARK_HOME=/opt/spark-2.3.0-bin-hadoop2.7
	export PATH=$SPARK_HOME/bin:$PATH

# start hadoop (wait http://localhost:50070/dfshealth.html "Safemode is off.")
start-dfs.sh
start-yarn.sh
hdfs dfs -mkdir -p /user/hsiehpinghan/spark-python-2
hdfs dfs -copyFromLocal /opt/spark-2.3.0-bin-hadoop2.7/LICENSE /user/hsiehpinghan/spark-python-2
hdfs dfs -chown -R hsiehpinghan:supergroup /user/hsiehpinghan

# ResourceManager site
http://localhost:8088

# hdfs site
http://localhost:50070

# spark web ui
http://localhost:4040

## WordCount test (https://spark.apache.org/docs/2.3.0/configuration.html)
# local
spark-submit --master local[*] --driver-memory 1g --executor-memory 1g --name WordCount /home/hsiehpinghan/git/python/spark-python-2/src/basic/WordCount.py
# yarn
HADOOP_CONF_DIR=/opt/hadoop-2.7.6/etc/hadoop spark-submit --master yarn --deploy-mode client --driver-memory 1g --executor-cores 2 --name WordCount /home/hsiehpinghan/git/python/spark-python-2/src/basic/WordCount.py

## run pyspark in eclipse (local mode)
Run -> External Tools -> External Tools Configurations -> Program
New launch configuration
# Main tag
Name: spark-submit local
Location: /opt/spark-2.3.0-bin-hadoop2.7/bin/spark-submit
Working Directory: ${project_loc}
Arguments: --master local[*] --driver-memory 1g --executor-memory 1g ${resource_loc}
# Environment tag
PYSPARK_DRIVER_PYTHON=/home/hsiehpinghan/anaconda2/bin/ipython
PYSPARK_PYTHON=/home/hsiehpinghan/anaconda2/envs/spark-python-2/bin/python

## run pyspark in eclipse (yarn mode)
Run -> External Tools -> External Tools Configurations -> Program
New launch configuration
# Main tag
Name: spark-submit yarn
Location: /opt/spark-2.3.0-bin-hadoop2.7/bin/spark-submit
Working Directory: ${project_loc}
Arguments: --master yarn --deploy-mode client --driver-memory 1g --executor-cores 2 ${resource_loc}
# Environment tag
HADOOP_CONF_DIR=/opt/hadoop-2.7.6/etc/hadoop
PYSPARK_DRIVER_PYTHON=/home/hsiehpinghan/anaconda2/bin/ipython
PYSPARK_PYTHON=/home/hsiehpinghan/anaconda2/envs/spark-python-2/bin/python

# download stumbleupon (https://www.kaggle.com/c/stumbleupon/data)

# download Bike-Sharing-Dataset
wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip -P /tmp/
unzip /tmp/Bike-Sharing-Dataset.zip -d /tmp/Bike-Sharing-Dataset/


