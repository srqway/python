# create anaconda virtual machine
https://www.anaconda.com/download/
sh /home/hsiehpinghan/Downloads/Anaconda3-5.1.0-Linux-x86_64.sh -b
/home/hsiehpinghan/anaconda3/bin/conda create --name spark-python-2 python=3.6 anaconda
/home/hsiehpinghan/anaconda3/bin/jupyter notebook --generate-config
vi /home/hsiehpinghan/.jupyter/jupyter_notebook_config.py
	c.NotebookApp.notebook_dir = '/home/hsiehpinghan/git/python/spark-python-2/notebook/'
cd /home/hsiehpinghan/anaconda3/bin
source activate spark-python-2

# start jupyter notebook
cd /home/hsiehpinghan/anaconda3/bin
source activate spark-python-2
jupyter lab

# set login without password
https://github.com/hsiehpinghan/example/blob/master/bash-shell-example/example/ssh

# download hadoop
wget http://apache.stu.edu.tw/hadoop/common/hadoop-2.7.6/hadoop-2.7.6.tar.gz -P /tmp/
tar xvfz /tmp/hadoop-2.7.6.tar.gz -C /opt/
vi /etc/profile
	export JAVA_HOME=/opt/jdk1.8.0_151
	export PATH=$JAVA_HOME/bin:$PATH
	export HADOOP_HOME=/opt/hadoop-2.7.6
	export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
vi /opt/hadoop-2.7.6/etc/hadoop/hadoop-env.sh
	export JAVA_HOME=/opt/jdk1.8.0_151
vi /opt/hadoop-2.7.6/etc/hadoop/core-site.xml 
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://localhost:9000</value>
        </property>
    </configuration>
vi /opt/hadoop-2.7.6/etc/hadoop/yarn-site.xml
    <configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
    </configuration>
cp /opt/hadoop-2.7.6/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.7.6/etc/hadoop/mapred-site.xml
vi /opt/hadoop-2.7.6/etc/hadoop/mapred-site.xml
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>
vi /opt/hadoop-2.7.6/etc/hadoop/hdfs-site.xml
	<configuration>
	    <property>
	        <name>dfs.replication</name>
	        <value>1</value>
	    </property>
	    <property>
	        <name>dfs.namenode.name.dir</name>
	        <value>file:///home/hsiehpinghan/Desktop/hdfs/namenode</value>
	    </property>
	    <property>
	        <name>dfs.datanode.data.dir</name>
	        <value>file:///home/hsiehpinghan/Desktop/hdfs/datanode</value>
	    </property>
	</configuration>
mkdir -p /home/hsiehpinghan/Desktop/hdfs/namenode
mkdir -p /home/hsiehpinghan/Desktop/hdfs/datanode
hdfs namenode -format

# download scala
wget https://www.scala-lang.org/files/archive/scala-2.11.12.tgz -P /tmp
tar xvfz /tmp/scala-2.11.12.tgz -C /opt/
vi /etc/profile
	export SCALA_HOME=/opt/scala-2.11.12
	export PATH=$SCALA_HOME/bin:$PATH

# download spark
wget http://apache.stu.edu.tw/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz -P /tmp
tar xvfz /tmp/spark-2.3.0-bin-hadoop2.7.tgz -C /opt/
cp /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties.template /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties
vi /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties
	log4j.rootCategory=WARN, console
vi /etc/profile
	export SPARK_HOME=/opt/spark-2.3.0-bin-hadoop2.7
	export PATH=$SPARK_HOME/bin:$PATH

# start hadoop (wait http://localhost:50070/dfshealth.html "Safemode is off.")
start-dfs.sh
start-yarn.sh

# ResourceManager site
http://localhost:8088

# hdfs site
http://localhost:50070

## test
hdfs dfs -mkdir -p /user/hsiehpinghan/spark-python-2
hdfs dfs -chown -R hsiehpinghan:supergroup /user/hsiehpinghan
hdfs dfs -copyFromLocal /opt/spark-2.3.0-bin-hadoop2.7/LICENSE /user/hsiehpinghan/spark-python-2
# local
pyspark --master local[*]
localFile = sc.textFile("file:///etc/passwd")
localFile.count()
hdfsFile = sc.textFile("hdfs://localhost:9000/user/hsiehpinghan/spark-python-2/LICENSE")
hdfsFile.count()
exit()
# yarn
HADOOP_CONF_DIR=/opt/hadoop-2.7.6/etc/hadoop pyspark --master yarn --deploy-mode client
hdfsFile = sc.textFile("hdfs://localhost:9000/user/hsiehpinghan/spark-python-2/LICENSE")
hdfsFile.count()
http://localhost:8088
exit()

