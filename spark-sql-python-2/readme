# download scala
wget https://www.scala-lang.org/files/archive/scala-2.11.12.tgz -P /tmp
tar xvfz /tmp/scala-2.11.12.tgz -C /opt/
vi /etc/profile
	export SCALA_HOME=/opt/scala-2.11.12
	export PATH=$SCALA_HOME/bin:$PATH

# download spark
wget http://apache.stu.edu.tw/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz -P /tmp
tar xvfz /tmp/spark-2.3.0-bin-hadoop2.7.tgz -C /opt/
cp /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties.template /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties
vi /opt/spark-2.3.0-bin-hadoop2.7/conf/log4j.properties
	log4j.rootCategory=WARN, console
vi /etc/profile
	export SPARK_HOME=/opt/spark-2.3.0-bin-hadoop2.7
	export PATH=$SPARK_HOME/bin:$PATH

# create anaconda virtual machine (https://repo.continuum.io/archive/)
sh /home/hsiehpinghan/Downloads/Anaconda2-5.1.0-Linux-x86_64.sh -b
/home/hsiehpinghan/anaconda2/bin/conda create --name python2.7 python=2.7 anaconda

## start jupyter notebook
export ANACONDA_HOME=/home/hsiehpinghan/anaconda2
export PATH=$ANACONDA_HOME/bin:$PATH
export PYSPARK_DRIVER_PYTHON=$ANACONDA_HOME/bin/ipython
export PYSPARK_PYTHON=$ANACONDA_HOME/envs/python2.7/bin/python
cd /home/hsiehpinghan/git/python/spark-sql-python-2/notebook
source activate python2.7
PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS="notebook" MASTER=local[*] pyspark


